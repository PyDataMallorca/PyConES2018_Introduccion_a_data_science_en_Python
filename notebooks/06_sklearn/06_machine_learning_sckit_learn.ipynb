{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Machine Learning con Python\n",
    "\n",
    "![Texto alternativo](./img/machine-learning.jpg)\n",
    "\n",
    "Una de las ramas de estudio que cada vez esta ganando más popularidad dentro de las ciencias de la computación es el * **aprendizaje automático o Machine Learning** *. Muchos de los servicios que utilizamos en nuestro día a día como google, gmail, netflix, spotify o amazon se valen de las herramientas que les brinda el Machine Learning para alcanzar un servicio cada vez más personalizado y lograr así ventajas competitivas sobre sus rivales.\n",
    "\n",
    "\n",
    "# Qué es Machine Learning?\n",
    "\n",
    "Pero, ¿qué es exactamente Machine Learning?. El Machine Learning es el diseño y estudio de las herramientas informáticas que utilizan la experiencia pasada para tomar decisiones futuras; es el estudio de programas que pueden aprenden de los datos. El objetivo fundamental del Machine Learning es generalizar, o inducir una regla desconocida a partir de ejemplos donde esa regla es aplicada. El ejemplo más típico donde podemos ver el uso del Machine Learning es en el filtrado de los correo basura o spam. Mediante la observación de miles de correos electrónicos que han sido marcados previamente como basura, los filtros de spam aprenden a clasificar los mensajes nuevos. El Machine Learning combina conceptos y técnicas de diferentes áreas del conocimiento, como las matemáticas, estadísticas y las ciencias de la computación; por tal motivo, hay muchas maneras de aprender la disciplina.\n",
    "\n",
    "\n",
    "## Tipos de Machine Learning\n",
    "\n",
    "El Machine Learning tiene una amplia gama de aplicaciones, incluyendo motores de búsqueda, diagnósticos médicos, detección de fraude en el uso de tarjetas de crédito, análisis del mercado de valores, clasificación de secuencias de ADN, reconocimiento del habla y del lenguaje escrito, juegos y robótica. Pero para poder abordar cada uno de estos temas es crucial en primer lugar distingir los distintos tipos de problemas de Machine Learning con los que nos podemos encontrar.\n",
    "\n",
    "## Aprendizaje supervisado\n",
    "\n",
    "En los problemas de aprendizaje supervisado se enseña o entrena al algoritmo a partir de datos que ya vienen etiquetados con la respuesta correcta. Cuanto mayor es el conjunto de datos más el algoritmo puede aprender sobre el tema. Una vez concluído el entrenamiento, se le brindan nuevos datos, ya sin las etiquetas de las respuestas correctas, y el algoritmo de aprendizaje utiliza la experiencia pasada que adquirió durante la etapa de entrenamiento para predecir un resultado. Esto es similar al método de aprendizaje que se utiliza en las escuelas, donde se nos enseñan problemas y las formas de resolverlos, para que luego podamos aplicar los mismos métodos en situaciones similares.\n",
    "\n",
    "## Aprendizaje no supervisado\n",
    "\n",
    "En los problemas de aprendizaje no supervisado el algoritmo es entrenado usando un conjunto de datos que no tiene ninguna etiqueta; en este caso, nunca se le dice al algoritmo lo que representan los datos. La idea es que el algoritmo pueda encontrar por si solo patrones que ayuden a entender el conjunto de datos. El aprendizaje no supervisado es similar al método que utilizamos para aprender a hablar cuando somos bebes, en un principio escuchamos hablar a nuestros padres y no entendemos nada; pero a medida que vamos escuchando miles de conversaciones, nuestro cerebro comenzará a formar un modelo sobre cómo funciona el lenguaje y comenzaremos a reconocer patrones y a esperar ciertos sonidos.\n",
    "\n",
    "## Aprendizaje por refuerzo\n",
    "\n",
    "En los problemas de aprendizaje por refuerzo, el algoritmo aprende observando el mundo que le rodea. Su información de entrada es el feedback o retroalimentación que obtiene del mundo exterior como respuesta a sus acciones. Por lo tanto, el sistema aprende a base de ensayo-error. Un buen ejemplo de este tipo de aprendizaje lo podemos encontrar en los juegos, donde vamos probando nuevas estrategias y vamos seleccionando y perfeccionando aquellas que nos ayudan a ganar el juego. A medida que vamos adquiriendo más practica, el efecto acumulativo del refuerzo a nuestras acciones victoriosas terminará creando una estrategia ganadora.\n",
    "\n",
    "\n",
    "# Nota Importante\n",
    "\n",
    "## Sobreentrenamiento\n",
    "\n",
    "Como mencionamos cuando definimos al Machine Learning, la idea fundamental es encontrar patrones que podamos generalizar para luego poder aplicar esta generalización sobre los casos que todavía no hemos observado y realizar predicciones. Pero también puede ocurrir que durante el entrenamiento solo descubramos casualidades en los datos que se parecen a patrones interesantes, pero que no generalicen. Esto es lo que se conoce con el nombre de **sobreentrenamiento o sobreajuste ó en inglés overfitting** .\n",
    "\n",
    "El sobreentrenamiento es la tendencia que tienen la mayoría de los algoritmos de Machine Learning a ajustarse a unas características muy específicas de los datos de entrenamiento que no tienen relación causal con la función objetivo que estamos buscando para generalizar. El ejemplo más extremo de un modelo sobreentrenado es un modelo que solo memoriza las respuestas correctas; este modelo al ser utilizado con datos que nunca antes ha visto va a tener un rendimiento azaroso, ya que nunca logró generalizar un patrón para predecir.\n",
    "\n",
    "\n",
    "    En aprendizaje automático, el sobreajuste (también es frecuente emplear el término en inglés overfitting) es el efecto de sobreentrenar un algoritmo de aprendizaje con unos ciertos datos para los que se conoce el resultado deseado. El algoritmo de aprendizaje debe alcanzar un estado en el que será capaz de predecir el resultado en otros casos a partir de lo aprendido con los datos de entrenamiento, generalizando para poder resolver situaciones distintas a las acaecidas durante el entrenamiento. Sin embargo, cuando un sistema se entrena demasiado (se sobreentrena) o se entrena con datos extraños, el algoritmo de aprendizaje puede quedar ajustado a unas características muy específicas de los datos de entrenamiento que no tienen relación causal con la función objetivo. Durante la fase de sobreajuste el éxito al responder las muestras de entrenamiento sigue incrementándose mientras que su actuación con muestras nuevas va empeorando. \n",
    "    \n",
    "![Texto alternativo](./img/cross_val_01.png)\n",
    "    \n",
    "\n",
    "## Como evitar el sobreentrenamiento\n",
    "\n",
    "Como mencionamos anteriormente, todos los modelos de Machine Learning tienen tendencia al sobreentrenamiento; es por esto que debemos aprender a convivir con el mismo y tratar de tomar medidas preventivas para reducirlo lo más posible. Las dos principales estrategias para lidiar son el sobreentrenamiento son: la retención de datos y la validación cruzada.\n",
    "\n",
    "En el primer caso, la idea es dividir nuestro conjunto de datos, en uno o varios conjuntos de entrenamiento y otro/s conjuntos de evaluación. Es decir, que no le vamos a pasar todos nuestros datos al algoritmo durante el entrenamiento, sino que vamos a retener una parte de los datos de entrenamiento para realizar una evaluación de la efectividad del modelo. Con esto lo que buscamos es evitar que los mismos datos que usamos para entrenar sean los mismos que utilizamos para evaluar. De esta forma vamos a poder analizar con más precisión como el modelo se va comportando a medida que más lo vamos entrenando y poder detectar el punto crítico en el que el modelo deja de generalizar y comienza a sobreajustarse a los datos de entrenamiento.\n",
    "\n",
    "**La validación cruzada** es un procedimiento más sofisticado que el anterior. En lugar de solo obtener una simple estimación de la efectividad de la generalización; la idea es realizar un análisis estadístico para obtener otras medidas del rendimiento estimado, como la media y la varianza, y así poder entender cómo se espera que el rendimiento varíe a través de los distintos conjuntos de datos. Esta variación es fundamental para la evaluación de la confianza en la estimación del rendimiento. La validación cruzada también hace un mejor uso de un conjunto de datos limitado; ya que a diferencia de la simple división de los datos en uno el entrenamiento y otro de evaluación; la validación cruzada calcula sus estimaciones sobre todo el conjunto de datos mediante la realización de múltiples divisiones e intercambios sistemáticos entre datos de entrenamiento y datos de evaluación.\n",
    "\n",
    "La validación cruzada o cross-validation esuna técnica utilizadapara evaluar los resultados de un análisisestadístico\n",
    "y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. Consiste en repetir y calcular la media aritmética obtenida de las medidas de evaluación sobre diferentes particiones. Se utiliza en entornos donde el objetivo principal es la predicción y se quiere estimar cómo de preciso es un modelo que se llevará a cabo a la práctica.\n",
    "Es una técnica muy utilizada en proyectos de inteligencia artificial para validar mode los generados. La siguiente figura muestra un esquema de esta técnica.\n",
    "\n",
    "\n",
    "![Texto alternativo](./img/cross_val_01.png)\n",
    "\n",
    "\n",
    "## Tipos de validaciones cruzadas\n",
    "\n",
    "### Validación cruzada de K iteraciones\n",
    "\n",
    "En la validación cruzada de K iteraciones o K-fold cross-validation los datos de muestra se dividen en K subconjuntos. Uno de los subconjuntos se utiliza como datos de prueba y el resto (K-1) como datos de entrenamiento. El proceso de validación cruzada es repetido durante k iteraciones, con cada uno de los posibles subconjuntos de datos de prueba. Finalmente se realiza la media aritmética de los resultados de cada iteración para obtener un único resultado. Este método es muy preciso puesto que evaluamos a partir de K combinaciones de datos de entre namiento y de prueba, pero aun así tiene una desventaja, y es que, a diferencia del método de retención, es lento des de el punto de vista computacional. En la práctica, la elección del número de iteraciones depende de la medida del conjunto de datos. Lo más común es utlizar la validación cruzada de 10 iteraciones (10-fold cross-validation).\n",
    "\n",
    "![Texto alternativo](./img/cross_val_02.png)\n",
    "\n",
    "\n",
    "#### Validación cruzada aleatoria\n",
    "Este método consiste al dividir aleatoriamente el conjunto de datos de entrenamiento y el conjunto de datos de\n",
    "prueba. Para cada división la función de aproximación se ajusta a partir de los datos de entrenamiento y calculalos valores de salida para el conjunto de datos de prueba. El resultado final se corresponde a la media aritmética de los valores obtenidos para las diferentes divisiones. La ventaja de este método es que la división de datos entrenamiento-prueba no depende del número de iteraciones. Pero, en cambio, con este método hay algunas muestras que quedan sin evaluar y otras que se evalúan más de una vez, es decir, los subconjuntos de prueba y entrenamiento se pueden solapar.\n",
    "\n",
    "![Texto alternativo](./img/cross_val_03.png)\n",
    "\n",
    "\n",
    "#### Validación cruzada dejando uno fuera\n",
    "La validación cruzada dejando uno fuera o Leave-one-out cross-validation (LOOCV) implica separar los datos de forma que para cada iteración tengamos una sola muestra para los datos de prueba y todo el resto conformando los datos de entrenamiento. La evaluación viene dada por el error, y en este tipo de validación cruzada el error es muy bajo, pero en cambio, a nivel computacional es muy costoso, puesto que se tienen que realizar un elevado número de iteraciones, tantas como N muestras tengamos y para cada una analizar los datos tanto de entrenamiento como de prueba.\n",
    "\n",
    "![Texto alternativo](./img/cross_val_04.png)\n",
    "\n",
    "\n",
    "#### Cálculo del error\n",
    "La evaluación de las diferentes validaciones cruzadas normalmente viene dada por el error obtenido en cada iteración, ahora bien, por cada uno de los métodos puede variar el número de iteraciones, según la elección del\n",
    "diseñador en función del número de datos total.\n",
    "\n",
    "#### Error de la validación cruzada de K iteraciones\n",
    "En cada una de las k iteraciones de este tipo de validación se realiza un cálculo de error. El resultado final lo obtenemos a partir de realizar la media aritmética de los $K$ valores de errores obtenidos, según la fórmula:\n",
    "\n",
    "\\begin{equation}\n",
    "   E = \\frac{1}{K}\\sum_{i=1}^{K}E_i\n",
    "\\end{equation}\n",
    "\n",
    "Es decir, se realiza el sumatorio de los $K$ valores de error y se divide entre el valor de $K$.\n",
    "\n",
    "#### Error de la validación cruzada aleatoria\n",
    "\n",
    "En la validación cruzada aleatoria a diferencia del método anterior, cogemos muestras al azar durante k iteraciones, aunque de igual manera, se realiza un cálculo de error para cada iteración. El resultado final también lo obtenemos a partir de realizar la media aritmética de los $K$ valores de errores obtenidos, según la misma fórmula\n",
    "\n",
    "\n",
    "#### Error de la validación cruzada dejando uno fuera\n",
    "En la validación cruzada dejando uno fuera se realizan tantas iteraciones como muestras $(N)$ tenga el conjunto de datos. De forma que para cada una de las N iteraciones se realiza un cálculo de error. El resultado final lo obtenemos realizando la media aritmética de los N valores de errores obtenidos, según la fórmula:\n",
    "\n",
    "\\begin{equation}\n",
    "   E = \\frac{1}{N}\\sum_{i=1}^{N}E_i\n",
    "\\end{equation}\n",
    "\n",
    "Donde se realiza el sumatorio de los $N$ valores de error y se divide entre el valor de $N$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Pasos para construir un modelo de machine learning\n",
    "\n",
    "Construir un modelo de Machine Learning, no se reduce solo a utilizar un algoritmo de aprendizaje o utilizar una librería de Machine Learning; sino que es todo un proceso que suele involucrar los siguientes pasos:\n",
    "\n",
    "**Recolectar los datos**. Podemos recolectar los datos desde muchas fuentes, podemos por ejemplo extraer los datos de un sitio web o obtener los datos utilizando una API o desde una base de datos. Podemos también utilizar otros dispositivos que recolectan los datos por nosotros; o utilizar datos que son de dominio público. El número de opciones que tenemos para recolectar datos no tiene fin!. Este paso parece obvio, pero es uno de los que más complicaciones trae y más tiempo consume.\n",
    "\n",
    "**Preprocesar los datos**. Una vez que tenemos los datos, tenemos que asegurarnos que tiene el formato correcto para nutrir nuestro algoritmo de aprendizaje. Es prácticamente inevitable tener que realizar varias tareas de preprocesamiento antes de poder utilizar los datos. Igualmente este punto suele ser mucho más sencillo que el paso anterior.\n",
    "\n",
    "**Explorar los datos**. Una vez que ya tenemos los datos y están con el formato correcto, podemos realizar un pre análisis para corregir los casos de valores faltantes o intentar encontrar a simple vista algún patrón en los mismos que nos facilite la construcción del modelo. En esta etapa suelen ser de mucha utilidad las medidas estadísticas y los gráficos en 2 y 3 dimensiones para tener una idea visual de como se comportan nuestros datos. En este punto podemos detectar valores atípicos que debamos descartar; o encontrar las características que más influencia tienen para realizar una predicción.\n",
    "\n",
    "**Entrenar el algoritmo**. Aquí es donde comenzamos a utilizar las técnicas de Machine Learning realmente. En esta etapa nutrimos al o los algoritmos de aprendizaje con los datos que venimos procesando en las etapas anteriores. La idea es que los algoritmos puedan extraer información útil de los datos que le pasamos para luego poder hacer predicciones.\n",
    "\n",
    "**Evaluar el algoritmo**. En esta etapa ponemos a prueba la información o conocimiento que el algoritmo obtuvo del entrenamiento del paso anterior. Evaluamos que tan preciso es el algoritmo en sus predicciones y si no estamos muy conforme con su rendimiento, podemos volver a la etapa anterior y continuar entrenando el algoritmo cambiando algunos parámetros hasta lograr un rendimiento aceptable.\n",
    "\n",
    "**Utilizar el modelo**. En esta ultima etapa, ya ponemos a nuestro modelo a enfrentarse al problema real. Aquí también podemos medir su rendimiento, lo que tal vez nos obligue a revisar todos los pasos anteriores.\n",
    "\n",
    "# Librerías de Python para machine learning\n",
    "\n",
    "Como siempre me gusta comentar, una de las grandes ventajas que ofrece Python sobre otros lenguajes de programación; es lo grande y prolifera que es la comunidad de desarrolladores que lo rodean; comunidad que ha contribuido con una gran variedad de librerías de primer nivel que extienden la funcionalidades del lenguaje. Para el caso de Machine Learning, las principales librerías que podemos utilizar son:\n",
    "\n",
    "# Scikit-Learn\n",
    "\n",
    "Scikit-learn es la principal librería que existe para trabajar con Machine Learning, incluye la implementación de un gran número de algoritmos de aprendizaje. La podemos utilizar para clasificaciones, extraccion de características, regresiones, agrupaciones, reducción de dimensiones, selección de modelos, o preprocesamiento. Posee una API que es consistente en todos los modelos y se integra muy bien con el resto de los paquetes científicos que ofrece Python. Esta librería también nos facilita las tareas de evaluación, diagnostico y validaciones cruzadas ya que nos proporciona varios métodos de fábrica para poder realizar estas tareas en forma muy simple.\n",
    "\n",
    "# Statsmodels\n",
    "\n",
    "Statsmodels es otra gran librería que hace foco en modelos estadísticos y se utiliza principalmente para análisis predictivos y exploratorios. Al igual que Scikit-learn, también se integra muy bien con el resto de los paquetes cientificos de Python. Si deseamos ajustar modelos lineales, hacer una análisis estadístico, o tal vez un poco de modelado predictivo, entonces Statsmodels es la librería ideal. Las pruebas estadísticas que ofrece son bastante amplias y abarcan tareas de validación para la mayoría de los casos.\n",
    "\n",
    "# PyMC\n",
    "\n",
    "pyMC es un módulo de Python que implementa modelos estadísticos bayesianos, incluyendo la cadena de Markov Monte Carlo(MCMC). pyMC ofrece funcionalidades para hacer el análisis bayesiano lo mas simple posible. Incluye los modelos bayesianos, distribuciones estadísticas y herramientas de diagnostico para la covarianza de los modelos. Si queremos realizar un análisis bayesiano esta es sin duda la librería a utilizar.\n",
    "\n",
    "# NTLK\n",
    "\n",
    "NLTK es la librería líder para el procesamiento del lenguaje natural o NLP por sus siglas en inglés. Proporciona interfaces fáciles de usar a más de 50 cuerpos y recursos léxicos, como WordNet, junto con un conjunto de bibliotecas de procesamiento de texto para la clasificación, tokenización, el etiquetado, el análisis y el razonamiento semántico.\n",
    "\n",
    "Obviamente, aquí solo estoy listando unas pocas de las muchas librerías que existen en Python para trabajar con problemas de Machine Learning, los invito a realizar su propia investigación sobre el tema.\n",
    "\n",
    "# Algoritmos más utilizados\n",
    "\n",
    "Los algoritmos que más se suelen utilizar en los problemas de Machine Learning son los siguientes:\n",
    "\n",
    " * Regresión Lineal\n",
    " * Regresión Logística\n",
    " * Arboles de Decision\n",
    " * Random Forest\n",
    " * SVM o Máquinas de vectores de soporte.\n",
    " * KNN o K vecinos más cercanos.\n",
    " * K-means\n",
    "\n",
    "\n",
    "# Lectura recomedada.\n",
    "\n",
    "![Texto alternativo](./img/lectura_recomendada.png)\n",
    "\n",
    "\n",
    "# Ejemplos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de Decisión \n",
    "\n",
    "Los Arboles de Decision son diagramas con construcciones lógicas, muy similares a los sistemas de predicción basados en reglas, que sirven para representar y categorizar una serie de condiciones que ocurren de forma sucesiva para la resolución de un problema en general de clasificación. Los Arboles de Decision están compuestos por nodos interiores, nodos terminales y ramas que emanan de los nodos interiores. Cada nodo interior en el árbol contiene una prueba de un atributo, y cada rama representa un valor distinto del atributo. Siguiendo las ramas desde el nodo raíz hacia abajo, cada ruta finalmente termina en un nodo terminal creando una segmentación de los datos.\n",
    "\n",
    "### ¿ Cómo funciona ?\n",
    "\n",
    "* El aprendizaje de árboles de decisión es sencillo, fácil de implementar y poderoso.\n",
    "* Un árbol recibe un objeto o situación descrita por un conjunto de atributos y regresa una decisión\n",
    "* Cada nodo interno corresponde a una prueba en el valor de uno de los atributos y las ramas están etiquetadas con los posibles valores de la prueba.\n",
    "* Cada hoja especifica el valor de la clase\n",
    "\n",
    "#### Complejidad\n",
    "\n",
    "* Para n atributos, hay $2^n$ filas y podemos considerar la salida como una función definida por $2^n$ bits \n",
    "* Con esto hay $2^{2^n}$ posibles funciones diferentes para $n$ atributos (para 6 atributos, hay $2\\times10^{19}$ ) \n",
    "* Por lo que tenemos que usar algún algoritmo ingenioso para encontrar una hipótesis consistente en un espacio de búsqueda tan grande\n",
    "\n",
    "#### Otras características\n",
    "+ Un ejemplo es descrito por los valores de los atributos y el valor del predicado meta (clase)\n",
    "+ Si el predicado es verdadero, entonces el ejemplo es positivo, sino el ejemplo es negativo\n",
    "+ **En caso de existir más clases, los ejemplos de una sola clase son positivos y el resto de los ejemplos son considerados negativos**\n",
    "\n",
    "\n",
    "#### Sigamos aprendiendo como funciona\n",
    "* Idea: Probar primero el atributo más “importante”\n",
    "* Este particiona los ejemplos creando diversos sub-conjuntos y cada subconjunto es un nuevo problema con menos ejemplos y un atributo menos\n",
    "* Este proceso recursivo tiene varios posibles resultados:\n",
    "   1. Si existen ejemplos positivos y negativos, escoge el mejor atributo\n",
    "   2. Si todos los ejemplos son positivos (o negativos), termina y regresa True (o False)\n",
    "   3. No quedan ejemplos, regresa un default con base en la clasificación mayoritaria de su nodo padre\n",
    "\n",
    "![Texto alternativo](./img/algortimo_ID3.png)\n",
    "\n",
    "#### ¿Cómo le hace?\n",
    "* La medida en ESCOGE-ATRIBUTO debe ser máxima cuando el atributo discrimine perfectamente ejemplos positivos y negativos, y mı́nima cuando el atributo no sea relevante\n",
    "\n",
    "* Una posibilidad es usar una medida basada en cantidad de información (basado en la teorı́a de Shannon y Weaver ’49)\n",
    "\n",
    "* La cantidad de información mide la (im)pureza en una colección arbitraria de ejemplos\n",
    "* La cantidad de información (medida en bits) recibida respecto a la ocurrencia de un evento es inversamente proporcional a la probabilidad de ocurrencia de dicho evento\n",
    "\n",
    "\n",
    "#### Ejemplo de Salir a Jugar Golf\n",
    "![Texto alternativo](./img/ejemploTree.png)\n",
    "\n",
    "### Entropia:\n",
    "\n",
    "Supongamos que temos 2 clases p y n.\n",
    "\n",
    "* En el caso de los árboles que queremos estimar las probabilidades de las respuestas (p ó n), lo hacemos con la proporción de ejemplos p y n\n",
    "\n",
    "* Si se tiene p ejemplos y n ejemplos done p + n = total de ejemplos, entonces la información requerida es:\n",
    "\n",
    "\\begin{equation}\n",
    "  I \\left(\\frac{p}{p+n},\\frac{n}{p+n}\\right) = -\\left(\\frac{p}{p+n}\\right)\\log_2 \\left(\\frac{p}{p+n}\\right) -  \\left(\\frac{n}{p+n}\\right)\\log_2 \\left(\\frac{n}{p+n}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "en su forma general:\n",
    "\n",
    "\\begin{equation}\n",
    "I(P(v_1),...,P(v_n)) = \\sum_{i=1}^{N_{clases}} P(v_i)\\log_2(P(v_i))\n",
    "\\end{equation}\n",
    "\n",
    "* Cada atributo A, divide a los ejemplos en subconjuntos $E_1,E_2, . . . ,E_v$ de acuerdo a los $v$ valores del atributo\n",
    "\n",
    "* Cada subconjunto $E_i$ tiene $p_i$ ejemplos y $n_i$ ejemplos, por lo que para cada rama  necesitamos: $I\\left( \\frac{p_i}{p_i + n_i}, \\frac{n_i}{p_i + n_i} \\right)$ para responder a una pregunta.\n",
    "\n",
    "* Por lo que en promedio, después de probar el atributo A, necesitamos:\n",
    "\n",
    "\\begin{equation}\n",
    "  E(A) = \\sum_{i=1}^{v} \\left( \\frac{p_i+n_i}{p+n} \\right) I\\left( \\frac{p_i}{p_i + n_i}, \\frac{n_i}{p_i + n_i} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "### Luego\n",
    "\n",
    "* La cantidad de informacion que ganamos al seleccionar un atributo esta dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "G(A) = I\\left( \\frac{p}{p + n}, \\frac{n}{p + n} \\right) - E(A)\n",
    "\\end{equation}\n",
    "\n",
    "+ La ganancia $G(A)$ de A me dice el número de bits que ahorramos para responder a la pregunta de la clase de un ejemplo, dado que conocemos el valor del atributo A.\n",
    "* Mide que tan bien un atributo separa a los ejemplos de entrenamiento de acuerdo a la clase\n",
    "* La funcion de evaluación escoge el atributo de mayor ganancia\n",
    "\n",
    "### Saquemos las cuentas para el ejemplo de la tabla de Golf:\n",
    "\n",
    "Primero calculemos la información requerida para la clasificación de las clases. Miramos la categoría clase donde tenemos P = 9 y N = 5 con un total de 14 ejemplos.\n",
    "\n",
    "\\begin{equation}\n",
    "I\\left( \\frac{p}{p + n}, \\frac{n}{p + n} \\right) = I \\left( \\frac{9}{14},\\frac{5}{14} \\right) = - \\frac{9}{14}\\log_2 \\left( \\frac{9}{12} \\right) - \\frac{5}{14}\\log_2 \\left(\\frac{5}{12} \\right) = 0.941\\ bits\n",
    "\\end{equation}\n",
    "\n",
    "#### Para el atributo Ambiente:\n",
    "Considerando el atributo **Ambiente**, con sus tres valores (v=3):\n",
    "\n",
    "***Soleado:*** $p_1 = 2$, $n_1 = 3$; $I(p_1,n_1) = 0.971\\ \\ bits$\n",
    "\n",
    "***Nublado:*** $p_2 = 4$, $n_2 = 0$; $I(p_2,n_2) = 0\\ \\ bits$\n",
    "\n",
    "***Lluvioso:*** $p_3 = 3$, $n_3 = 2$; $I(p_3,n_3) = 0.971\\ \\ bits$\n",
    "\n",
    "Eentropía para el atributo Ambiente es:\n",
    "\n",
    "$E(Ambiente) = \\left( \\frac{5}{14} \\right) I(p_1,n_1) + \\left( \\frac{4}{14} \\right) I(p_2,n_2) + \\left( \\frac{5}{14} \\right) I(p_3,n_3) = 0.694\\ bits$\n",
    "\n",
    "#### Para Humedad (v=2):\n",
    "***Alta:*** $p_1 = 3$, $n_1 = 4$; $I(p_1,n_1) = 0.985\\ \\ bits$\n",
    "\n",
    "***Normal:*** $p_2 = 6$, $n_2 = 1$; $I(p_2,n_2) = 0.592\\ \\ bits$\n",
    "\n",
    "$E(Humedad) = 0.798\\ bits$\n",
    "\n",
    "#### Para Viento (v=2):\n",
    "***No:*** $p_1 = 6$, $n_1 = 2$; $I(p_1,n_1) = 0.811\\ \\ bits$\n",
    "\n",
    "***Si:*** $p_2 = 3$, $n_2 = 3$; $I(p_2,n_2) = 1.0\\ \\ bits$\n",
    "\n",
    "$E(Viento) = 0.892\\ bits$\n",
    "\n",
    "\n",
    "#### Para Temperatura: \n",
    "$E(Temperatura) = 0.911\\ bits$\n",
    "\n",
    "#### Ganancias G(A) para cada atributo\n",
    "\n",
    "Las ganancias son entonces:\n",
    "\n",
    "$G(Ambiente) = 0.246\\ bits $ (MAX)\n",
    "\n",
    "$G(Humedad) = 0.151\\ bits$\n",
    "\n",
    "$G(Viento) = 0.048\\ bits$\n",
    "\n",
    "$G(Temperatura) = 0.029\\ bits$\n",
    "\n",
    "Por lo que ID3 escoge el atributo Ambiente como nodo raíz y procede a realizar el mismo proceso con los ejemplos de cada rama\n",
    "\n",
    "\n",
    "#### Entonces:\n",
    "Para Ambiente tenemos tres subconjuntos: **soleado** (2P, 3N), **nublado** (4P, 0N), **lluvioso** (3P, 2N). \n",
    "\n",
    "Para nublado, no tenemos que hacer nada, mas que asignarle la clase P (mirara Tabla)\n",
    "\n",
    "Por ejemplo, para soleado haríamos el mismo proceso:\n",
    "\n",
    "$G(Humedad) = 0.97 - [(3/5)0 + (2/5)0] = 0.97\\ bits$ (MAX)\n",
    "\n",
    "$G(Temperatura) = 0.97 - [(2/5)0 + (2/5)1 + (1/5)0] = 0.570\\ bits$\n",
    "\n",
    "$G(Viento) = 0.97 - [(2/5)1 + (3/5)0.918] = 0.019\\ bits$\n",
    "\n",
    "Con el arbol construido, podemos preguntar si está bién\n",
    "jugar el sabado en la mañana con ambiente soleado, \n",
    "temperatura alta, humedad alta y con viento, a lo cual el\n",
    "árbol me responde que no\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de Decisión para Iris DataSet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "# Arboles de Decision\n",
    "\n",
    "# importando pandas, numpy y matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('../../data/04_01_iris.csv')\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labores de preprocesamiento\n",
    "\n",
    "Sin embargo, scikit-learn no acepta cadenas como parámetros de las funciones, todo deben de ser números. Para ello, nos podemos valer del objeto sklearn.preprocessing.LabelEncoder, que nos transforma automáticamente las cadenas a números. La forma en que se utiliza es la siguiente:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#? pd.tools.plotting.scatter_matrix()\n",
    "#? preprocessing.LabelEncoder()\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(iris_df['species'])\n",
    "iris_df['species_cod'] = le.transform(iris_df['species'])\n",
    "\n",
    "iris_df\n",
    "#le.inverse_transform(iris_df.species_cod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podéis observar, primero se crea el LabelEncoder y luego se \"entrena\" mediante el método fit. Para un LabelEncoder, \"entrenar\" el modelo es decidir el mapeo que vimos anteriormente, en este caso:\n",
    "\n",
    "   * Iris-setosa -> 0\n",
    "   * Iris-versicolor -> 1\n",
    "   * Iris-virginica -> 2\n",
    "\n",
    "Una vez entrenado, utilizando el método transform del LabelEncoder, podremos transformar cualquier ndarray que queramos (hubiéramos tenido un error si alguna de las etiquetas de test no estuviera en train). Esta estructura (método fit más método transform o predict) se repite en muchos de los objetos de scikit-learn.\n",
    "\n",
    "Hay muchas más tareas de preprocesamiento que se pueden hacer en scikit-learn. **Consulta el paquete sklearn.preprocessing**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns; \n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "sns.pairplot(iris, hue=\"species\")\n",
    "\n",
    "# Nota--> ayuda sobre alguna función...\n",
    "#? sns.pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = iris_df.drop(['species','species_cod'],axis=1)\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_target = iris_df.species_cod\n",
    "iris_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separamos los Datos.... Entrenamiento y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separamos los Datos.... Entrenamiento y test\n",
    "#?  train_test_split()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_target,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=102,\n",
    "                                                    shuffle =True)\n",
    "\n",
    "print('Set de datos para Entrenamiento =',len(X_train))\n",
    "print('Set de datos para Test',len(X_test))\n",
    "print('Total',len(X_test)+len(X_train))\n",
    "print('Data Shape=',X_test.shape)\n",
    "print('Target Shape =',y_test.shape)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Árboles de Decisión y Random Forest Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "arbol = DecisionTreeClassifier(max_depth=None, criterion='entropy', random_state=4)\n",
    "#arbol = DecisionTreeClassifier()\n",
    "\n",
    "#? DecisionTreeClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? cross_val_score()\n",
    "cross_val_score(arbol, iris_data, iris_target)\n",
    "arbol.fit(X_train,y_train)\n",
    "\n",
    "print (\"Score with data Tes\",arbol.score(X_test,y_test))\n",
    "print (\"Score with data Train\",arbol.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "iris_target_names = ['setosa', 'versicolor', 'virginica']\n",
    "iris_feature_names =['sepal length (cm)',\n",
    " 'sepal width (cm)',\n",
    " 'petal length (cm)',\n",
    " 'petal width (cm)']\n",
    "\n",
    "export_graphviz(arbol,out_file='arbol.dot',class_names=iris_target_names,\n",
    "                feature_names=iris_feature_names,\n",
    "                impurity=False,filled=True\n",
    "               )\n",
    "\n",
    "with open('arbol.dot') as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cara = iris_data.shape[1]\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.barh(range(cara),arbol.feature_importances_)\n",
    "plt.yticks(np.arange(cara),iris_feature_names,fontsize=32)\n",
    "plt.xlabel('Importancia de las Caracteristicas',fontsize=42)\n",
    "plt.ylabel('Caracteristicas',fontsize=45)\n",
    "plt.xticks(fontsize=32)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "p = arbol.predict(X_test)\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, p))\n",
    "print ('\\nConfusion Matrix:\\n', confusion_matrix(y_test, p))\n",
    "print ('\\nClassification Report:', classification_report(y_test, p))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Alguna Predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# Alguna Predicción....\n",
    "prediccion = arbol.predict([[4.3,2.6,3.4,1.3]])\n",
    "#prediccion = arbol.predict(X_test)\n",
    "print(prediccion, le.inverse_transform(prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ind = 78\n",
    "print(iris_data.iloc[ind])\n",
    "print('specie',iris_target.iloc[ind], le.inverse_transform(iris_target.iloc[ind]))\n",
    "x_new = iris_data.iloc[ind]\n",
    "\n",
    "print('\\n======== PREDICTION ========')\n",
    "prediction    = arbol.predict([x_new.values])\n",
    "prediction_pb = arbol.predict_proba([x_new.values])\n",
    "print('Specie prediction',prediction, le.inverse_transform(prediction))\n",
    "print('Probability Specie prediction',prediction_pb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------- Random Forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#?RandomForestClassifier()\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100, criterion='entropy', random_state=0, verbose = 2, n_jobs=3)\n",
    "forest.fit(X_train,y_train)\n",
    "\n",
    "#you can tune parameter such as:\n",
    "# - n_job (how many cores)(n_job=-1 => all cores)\n",
    "# - max_depth\n",
    "# - max_feature\n",
    "\n",
    "\n",
    "print('acc for training data: {:.2f}'.format(forest.score(X_train,y_train)))\n",
    "print('acc for test data: {:.2f}'.format(forest.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alguna Predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ind = 78\n",
    "print(iris_data.iloc[ind])\n",
    "print('specie',iris_target.iloc[ind], le.inverse_transform(iris_target.iloc[ind]))\n",
    "x_new = iris_data.iloc[ind]\n",
    "\n",
    "print('\\n======== PREDICTION ========')\n",
    "prediction = forest.predict([x_new.values])\n",
    "prediction_pb = forest.predict_proba([x_new.values])\n",
    "print('Specie prediction',prediction, le.inverse_transform(prediction))\n",
    "print('Probability Specie prediction',prediction_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "p = forest.predict(X_test)\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, p))\n",
    "print ('\\nConfusion Matrix:\\n', confusion_matrix(y_test, p))\n",
    "print ('\\nClassification Report:', classification_report(y_test, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   K-Nearest Neighbors Classification \n",
    "\n",
    "## k vecinos más próximos\n",
    "\n",
    "El método de los ** k vecinos más cercanos ** (en inglés, k-nearest neighbors, abreviado $k-nn$) es un método de clasificación supervisada (Aprendizaje, estimación basada en un conjunto de entrenamiento y prototipos) que sirve para estimar la función de densidad $F(x|Cj)$ de las predictoras $x$ por cada clase $C_{j}$.\n",
    "\n",
    "Este es un método de clasificación no paramétrico, que estima el valor de la función de densidad de probabilidad o directamente la probabilidad a posteriori de que un elemento $x$ pertenezca a la clase $C_{j}$ a partir de la información proporcionada por el conjunto de prototipos. \n",
    "\n",
    "En el proceso de aprendizaje no se hace ninguna suposición acerca de la distribución de las variables predictoras.\n",
    "\n",
    "En el reconocimiento de patrones, el algoritmo $k-nn$ es usado como método de clasificación de objetos (elementos) basado en un entrenamiento mediante ejemplos cercanos en el espacio de los elementos.\n",
    "\n",
    "\n",
    "# Algoritmo\n",
    "\n",
    "Los ejemplos de entrenamiento son vectores en un espacio característico multidimensional, cada ejemplo está descrito en términos de $p$ atributos considerando $q$ clases para la clasificación. Los valores de los atributos del $i-ésimo$ ejemplo (donde $1\\leq i\\leq n$) se representan por el vector $\\vec p$;  p-dimensional\n",
    "\n",
    "entonces: \n",
    "\n",
    "\\begin{equation}\n",
    "x_{i}=(x_{1i},x_{2i},...,x_{pi})\\in X\n",
    "\\end{equation}\n",
    "\n",
    "El espacio es particionado en regiones, por localizaciones y etiquetas de los ejemplos de entrenamiento. Un punto en el espacio es asignado a la clase  $C$ si esta es la clase más frecuente entre los k ejemplos de entrenamiento más cercano. \n",
    "\n",
    "Generalmente se usa la distancia euclidiana.\n",
    "\n",
    "\\begin{equation}\n",
    "d(x_{i},x_{j})={\\sqrt {\\sum _{r=1}^{p}(x_{ri}-x_{rj})^{2}}}\n",
    "\\end{equation}\n",
    "\n",
    "=========================================================================================\n",
    "\n",
    "![Texto alternativo](./img/knn_img.png)\n",
    "\n",
    "**Ejemplo del algoritmo $k-nn$**. El ejemplo que se desea clasificar es el círculo verde. Para k = 3 este es clasificado con la clase triángulo, ya que hay solo un cuadrado y 2 triángulos, dentro del círculo que los contiene. Si k = 5 este es clasificado con la clase cuadrado, ya que hay 2 triángulos y 3 cuadrados, dentro del círculo externo.\n",
    "\n",
    "============================================================================================\n",
    "\n",
    "La fase de entrenamiento del algoritmo consiste en almacenar los vectores característicos y las etiquetas de las clases de los ejemplos de entrenamiento. En la fase de clasificación, la evaluación del ejemplo (del que no se conoce su clase) es representada por un vector en el espacio característico. Se calcula la distancia entre los vectores almacenados y el nuevo vector, y se seleccionan los $k$ ejemplos más cercanos. El nuevo ejemplo es clasificado con la clase que más se repite en los vectores seleccionados.\n",
    "\n",
    "Este método supone que los vecinos más cercanos nos dan la mejor clasificación y esto se hace utilizando todos los atributos; el problema de dicha suposición es que es posible que se tengan muchos atributos irrelevantes que dominen sobre la clasificación: dos atributos relevantes perderían peso entre otros veinte irrelevantes.\n",
    "\n",
    "Para corregir el posible sesgo se puede asignar un peso a las distancias de cada atributo, dándole así mayor importancia a los atributos más relevantes. \n",
    "\n",
    "### Vecinos más cercanos con distancia ponderada\n",
    "Se puede ponderar la contribución de cada vecino de acuerdo a la distancia entre él y el ejemplar a ser clasificado $x_{q}$, dando mayor peso a los vecinos más cercanos. Por ejemplo podemos ponderar el voto de cada vecino de acuerdo al cuadrado inverso de sus distancias\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "{\\hat {f}}(x_{q})\\leftarrow argmax_{v\\in V}\\sum _{i=1}^{k}w_{i}\\delta (v,f(x_{i}))\n",
    "\\end{equation}\n",
    "\n",
    "donde\n",
    "\n",
    "\\begin{equation}\n",
    " w_{i}\\equiv {\\frac {1}{d(x_{q},x_{i})^{2}}}\n",
    "\\end{equation}\n",
    " \n",
    "De esta manera se ve que no hay riesgo de permitir a todos los ejemplos entrenamiento contribuir a la clasificación de $ x_{q}$, ya que al ser muy distantes no tendrían peso asociado. La desventaja de considerar todos los ejemplos seria su lenta respuesta (método global). Se quiere siempre tener un método local en el que solo los vecinos más cercanos son considerados.\n",
    "\n",
    "Esta mejora es muy efectiva en muchos problemas prácticos. Es robusto ante los ruidos de datos y suficientemente efectivo en conjuntos de datos grandes. Se puede ver que al tomar promedios ponderados de los $k$ vecinos más cercanos, el algoritmo puede evitar el impacto de ejemplos con ruido aislados.\n",
    "\n",
    "\n",
    "Otra posibilidad consiste en tratar de determinar o ajustar los pesos con ejemplos conocidos de entrenamiento. \n",
    "\n",
    "Finalmente, antes de asignar pesos es recomendable identificar y eliminar los atributos que se consideran irrelevantes.\n",
    "\n",
    "\n",
    "### Resumen:\n",
    "\n",
    "1. Calculate distance\n",
    "2. Find closest neighbors\n",
    "3. Vote for labels\n",
    "\n",
    "![Texto alternativo](./img/knn_img_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Create two lists for training and test accuracies\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Define a range of 1 to 10 (included) neighbors to be tested\n",
    "neighbors_settings = range(1,10)\n",
    "\n",
    "# Loop with the KNN through the different number of neighbors to determine the most appropriate (best)\n",
    "for n_neighbors in neighbors_settings:\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, algorithm='auto', weights='uniform')\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "# Visualize results - to help with deciding which n_neigbors yields the best results (n_neighbors=6, in this case)\n",
    "plt.plot(neighbors_settings, training_accuracy, label='Accuracy of the training set', marker='o')\n",
    "plt.plot(neighbors_settings, test_accuracy, label='Accuracy of the test set', marker='o')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=9,weights='uniform', algorithm='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_train, y_train))\n",
    "print(clf.score(X_test, y_test))\n",
    "#? KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Alguna Predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ind = 78\n",
    "print(iris_data.iloc[ind])\n",
    "print('specie',iris_target.iloc[ind], le.inverse_transform(iris_target.iloc[ind]))\n",
    "x_new = iris_data.iloc[ind]\n",
    "\n",
    "a = np.array([[4.3,2.6,3.4,1.3]])\n",
    "print('\\n======== PREDICTION ========')\n",
    "prediction = clf.predict(a)\n",
    "prediction\n",
    "prediction_pb = clf.predict_proba([x_new.values])\n",
    "print('Specie prediction',prediction, le.inverse_transform(prediction))\n",
    "print('Probability Specie prediction',prediction_pb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "p = clf.predict(X_test)\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, p))\n",
    "print ('\\nConfusion Matrix:\\n', confusion_matrix(y_test, p))\n",
    "print ('\\nClassification Report:', classification_report(y_test, p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Support Vector Machine on Iris Flower Dataset  \n",
    "\n",
    "####  Máquinas de vectores de soporte\n",
    "\n",
    "Las máquinas de soporte vectorial, máquinas de vectores de soporte o máquinas de vector soporte (Support Vector Machines, SVMs) son un conjunto de algoritmos de aprendizaje supervisado desarrollados por Vladimir Vapnik y su equipo en los laboratorios AT&T.\n",
    "\n",
    "Estos métodos están propiamente relacionados con problemas de clasificación y regresión. \n",
    "El método consiste en que, dado un conjunto de ejemplos de entrenamiento (de muestras) podemos etiquetar las clases y entrenar una SVM para construir un modelo que prediga la clase de una nueva muestra. Intuitivamente, una SVM es un modelo que representa a los puntos de muestra en el espacio, separando las clases a 2 espacios lo más amplios posibles mediante un hiperplano de separación definido como el vector entre los 2 puntos, de las 2 clases, más cercanos al que se llama vector soporte. Cuando las nuevas muestras se ponen en correspondencia con dicho modelo, en función de los espacios a los que pertenezcan, pueden ser clasificadas a una o la otra clase.\n",
    "\n",
    "    Más formalmente, una SVM construye un hiperplano o conjunto de hiperplanos en un espacio de dimensionalidad muy alta (o incluso infinita) que puede ser utilizado en problemas de clasificación o regresión. Una buena separación entre las clases permitirá una clasificación correcta. \n",
    "    \n",
    "### Idea básica\n",
    "\n",
    "La SVM busca un hiperplano que separe de forma óptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior.\n",
    "\n",
    "En ese concepto de \"separación óptima\" es donde reside la característica fundamental de las SVM: este tipo de algoritmos buscan el hiperplano que tenga la máxima distancia (margen) con los puntos que estén más cerca de él mismo. Por eso también a veces se les conoce a las SVM como clasificadores de margen máximo. De esta forma, los puntos del vector que son etiquetados con una categoría estarán a un lado del hiperplano y los casos que se encuentren en la otra categoría estarán al otro lado.\n",
    "\n",
    "En la literatura de los SVMs, se llama atributo a la variable predictora y característica a un atributo transformado que es usado para definir el hiperplano. La elección de la representación más adecuada del universo estudiado, se realiza mediante un proceso denominado selección de características.\n",
    "\n",
    "Al vector formado por los puntos más cercanos al hiperplano se le llama vector de soporte.\n",
    "\n",
    "Los modelos basados en SVMs están estrechamente relacionados con las redes neuronales. Usando una función kernel, resultan un método de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptrón multicapa. \n",
    "\n",
    "\n",
    "### Espacios inducidos por la función Kernel\n",
    "\n",
    "Debido a las limitaciones computacionales de las máquinas de aprendizaje lineal estas no\n",
    "pueden ser utilizadas en la mayoría de las aplicaciones del mundo real. La representación por\n",
    "medio del Kernel ofrece una solución alternativa a este problema, proyectando la información\n",
    "a un espacio de características de mayor dimensión el cual aumenta la capacidad computacional de la máquinas de aprendizaje lineal. La forma más común en que las máquinas de\n",
    "aprendizaje lineales aprenden una función objetivo es cambiando la representación de la función, esto es similar a mapear el espacio de entradas $X$ a un nuevo espacio de características $F = \\{\\phi(x)|x ∈ X\\}$. Esto es:\n",
    "\n",
    "$x = \\{ x_1 , x_2,..., x_n \\} \\rightarrow \\phi(x) = \\{ \\phi(x)_1 , \\phi(x)_2 ,..., \\phi(x)_n \\}$\n",
    "\n",
    "En la figura siguiente se muestra un mapeo de un espacio de entradas de dos dimensiones a un\n",
    "espacio de características de dos dimensiones, donde la información no puede ser separada\n",
    "por una máquina lineal en el espacio de entradas mientras que en el espacio de características\n",
    "esto resulta muy sencillo.\n",
    "\n",
    "\n",
    "\n",
    "![Texto alternativo](./img/smv_2_img.jpg)\n",
    "\n",
    "\n",
    "Definición: Un Kernel $K$ es una función, tal que para todo $x_i,x_j \\in X$\n",
    "\n",
    "\\begin{equation}\n",
    " K(x_i, x_j) = \\langle \\phi (x_i) · \\phi (x_j) \\rangle = \\sum_{i=1}^{l} \\phi^T_{i}(x_i)\\phi_i(x_j)\n",
    "\\end{equation}\n",
    "\n",
    "donde $\\phi$ es un mapeo del espacio de entradas $X$ al espacio de características $F$.\n",
    "El uso de la función kernel hace posible realizar el mapeo de la información de entrada\n",
    "$(x_i, x_j)$ al espacio de características $(\\phi (x_i) ,  \\phi_i (x_j))$ de forma implícita y entrenar a la máquina lineal en dicho espacio. \n",
    "\n",
    "Luego mediante distintos algoritmos de optimización que permita un margen máximo (distancia) entre los elementos de las dos categorías que nos permite entrenar nuestro modelo.\n",
    "\n",
    "## Tipos de funciones Kernel (Núcleo)\n",
    "\n",
    "Polinomial-homogénea: $K(x_i, x_j) = (x_i·x_j)^n$\n",
    "\n",
    "Perceptron: $K(xi, xj)= || xi-xj ||$\n",
    "\n",
    "Función de base radial Gaussiana: separado por un hiperplano en el espacio transformado. $K(x_i, x_j)=e^{-\\frac{(x_i-x_j)^2}{2\\sigma^2}}$\n",
    "\n",
    "Sigmoid: $K(x_i, x_j)=tanh(x_i· x_j−\\phi)$\n",
    "\n",
    "\n",
    "### Ejemplo en 2–dimensiones\n",
    "\n",
    "![Texto alternativo](./img/smv_1_img.jpg)\n",
    "\n",
    "===========================================================================================\n",
    " \n",
    "Fig: H1 no separa las clases. H2 las separa, pero solo con un margen pequeño. H3 las separa con el margen máximo.\n",
    "\n",
    "=============================================================================================\n",
    "\n",
    "En un ejemplo idealizado para 2-dimensiones, la representación de los datos a clasificar se realiza en el plano x-y. El algoritmo SVM trata de encontrar un hiperplano 1-dimensional (en el ejemplo que nos ocupa es una línea) que une a las variables predictoras y constituye el límite que define si un elemento de entrada pertenece a una categoría o a la otra.\n",
    "\n",
    "Existe un número infinito de posibles hiperplanos (líneas) que realicen la clasificación pero, ¿cuál es la mejor y cómo la definimos?\n",
    "Hay infinitos hiperplanos posibles\n",
    "H1 no separa las clases. H2 las separa, pero solo con un margen pequeño. H3 las separa con el margen máximo.\n",
    "\n",
    "La mejor solución es aquella que permita un margen máximo entre los elementos de las dos categorías.\n",
    "\n",
    "Se denominan vectores de soporte a los puntos que conforman las dos líneas paralelas al hiperplano, siendo la distancia entre ellas (margen) la mayor posible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model=SVC()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ? cross_val_score()\n",
    "cross_val_score(model, iris_data, iris_target)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print (\"Score with data Tes\",model.score(X_test,y_test))\n",
    "print (\"Score with data Train\",model.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Clasification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "p = clf.predict(X_test)\n",
    "\n",
    "print ('Accuracy:', accuracy_score(y_test, p))\n",
    "print ('\\nConfusion Matrix:\\n', confusion_matrix(y_test, p))\n",
    "print ('\\nClassification Report:', classification_report(y_test, p))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Alguna Predicción "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "ind = 78\n",
    "print(iris_data.iloc[ind])\n",
    "print('specie',iris_target.iloc[ind], le.inverse_transform(iris_target.iloc[ind]))\n",
    "x_new = iris_data.iloc[ind]\n",
    "\n",
    "print('\\n======== PREDICTION ========')\n",
    "prediction = model.predict([x_new.values])\n",
    "print('Specie prediction',prediction, le.inverse_transform(prediction))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Diferentes  Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: data to base x-axis meshgrid on\n",
    "    y: data to base y-axis meshgrid on\n",
    "    h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: matplotlib axes object\n",
    "    clf: a classifier\n",
    "    xx: meshgrid ndarray\n",
    "    yy: meshgrid ndarray\n",
    "    params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "# we create an instance of SVM and fit out data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 0.1  # SVM regularization parameter\n",
    "models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "          svm.SVC(kernel='poly', degree=3, C=C))\n",
    "models = (clf.fit(X, y) for clf in models)\n",
    "\n",
    "# title for the plots\n",
    "titles = ('SVC with linear kernel',\n",
    "          'LinearSVC (linear kernel)',\n",
    "          'SVC with RBF kernel',\n",
    "          'SVC with polynomial (degree 3) kernel')\n",
    "\n",
    "# Set-up 2x2 grid for plotting.\n",
    "fig, sub = plt.subplots(2, 2)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.2)\n",
    "plt.rcParams['figure.figsize'] = (20, 16)\n",
    "\n",
    "X0, X1 = X[:, 0], X[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(models, titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.4)\n",
    "    ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=80, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicios:\n",
    "\n",
    "Replicar estas técnicas de Clasificación con los siguientes DataSet:\n",
    "\n",
    "1.  ./data/06_breast-cancer-wisconsin-data.csv\n",
    "2. ./data/06_diabetes.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
